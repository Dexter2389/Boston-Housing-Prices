{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Boston Housing Prices Demo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By Saurabh Ghanekar from Nimblebox Inc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Today in this demo, we will see how to build our own machine learning model to predict housing prices in Boston using the famous Boston Housing Price dataset based on the different features described in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Installing Dependencies**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the packages that we will be using are pre-installed on the Nimblebox platform. But still, the latest version of the following packages should be installed in your instance.\n",
    "\n",
    "* TensorFlow 2.x\n",
    "* Pandas\n",
    "* Matplotlib\n",
    "* Numpy\n",
    "* Seaborn\n",
    "* Scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Loading the Dataset**\n",
    "In this dataset, each row describes a Boston town or suburb. There are 506 rows and 13 attributes (features) with a target column (price). We will use pandas and scikit-learn to load and explore the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston_dataset = load_boston()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use `DESCR` to know the description of each column name and display our dataset in a nutshell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(boston_dataset.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to perform exploratory data analysis, we need to convert our dataset into a Pandas Dataframe and print the head of our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame(boston_dataset.data)\n",
    "dataset.columns = boston_dataset.feature_names\n",
    "\n",
    "print(\"Original Dataframe\")\n",
    "print(\"[INFO] dataset type: {}\".format(type(dataset)))\n",
    "print(\"[INFO] dataset shape: {}\".format(dataset.shape))\n",
    "\n",
    "# Insert the target column in our main dataframe\n",
    "dataset[\"PRICE\"] = boston_dataset.target\n",
    "\n",
    "print(\"\\nDataframe with target column\")\n",
    "print(\"[INFO] dataset type: {}\".format(type(dataset)))\n",
    "print(\"[INFO] dataset shape: {}\".format(dataset.shape))\n",
    "\n",
    "print(\"\\n\",dataset.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Analysis of Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Statistical Summary**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that our dataframe is ready, we will use `describe()` function to understand the statistical summary of the dataset. This function shows count, min, max, mean and standard deviation for each column of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Correlation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding a correlation between attributes is a highly useful way to check for patterns in the dataset. It helps us to find statistical relations between different attributes of our dataset. The output of each of these correlation functions falls within the range [-1, 1].\n",
    "\n",
    "* 1 - Positively correlated\n",
    "* -1 - Negatively correlated\n",
    "* 0 - Not correlated\n",
    "\n",
    "We will use `corr()` function to compute the correlation between attributes and use `heatmap()` function to visualize the correlation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation between attributes\n",
    "\n",
    "print(dataset.corr())\n",
    "sns.heatmap(dataset.corr())\n",
    "plt.savefig(\"heatmap_correlation.png\")\n",
    "plt.show()\n",
    "plt.clf()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pandas `corr()` function has different methods to find correlations like Pearson Correlation, Kendall Correlation or Spearman Correlation. The default method is to find correlations is Pearson Correlation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Missing Values**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, in a dataset, we will have missing values such as `NaN` or an empty string in a cell. We need to take care of these missing values so that our machine learning model doesn’t break. To handle missing values, there are three approaches followed.\n",
    "\n",
    "* Replace the missing value with a large negative number (e.g. -999).\n",
    "* Replace the missing value with mean of the column.\n",
    "* Replace the missing value with median of the column.\n",
    "\n",
    "To find if a column in our dataset has missing values, you can use `pd.isnull(df).any()` which returns a boolean for each column in the dataset that tells if the column contains any missing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.isnull(dataset).any())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turns out, our Boston Housing Prices Dataset doesn’t have any missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Visualizing the Dataset**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualization of data allows trends and patterns to be more easily seen. We will use Box Plot, Density Plot, and Scatter Plot to visualize our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the dataset\n",
    "import random\n",
    "import os\n",
    "\n",
    "sns.set(color_codes=True)\n",
    "colors = [\"y\", \"b\", \"g\", \"r\"]\n",
    "\n",
    "cols = list(dataset.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Box Plot**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A box-whisker plot is a univariate plot used to visualize a data distribution.\n",
    "\n",
    "* The ends of whiskers are the maximum and minimum range of data distribution.\n",
    "* The central line in the box is the median of the entire data distribution.\n",
    "* The right and left edges in the box are the medians of the data distribution to the right and left from the central median, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box Plot\n",
    "if not os.path.exists(\"plots/box_plot\"):\n",
    "    os.makedirs(\"plots/box_plot\")\n",
    "\n",
    "# draw a boxplot with vertical orientation\n",
    "for i, col in enumerate(cols):\n",
    "    sns.boxplot(dataset[col], color=random.choice(colors), orient=\"v\")\n",
    "    plt.savefig(\"plots/box_plot/box_plot_\" + str(i) + \".png\")\n",
    "    plt.show()\n",
    "    plt.clf()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the box plots, we could see that there are outliers in the dataset for different attributes in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Density Plot**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Density plot is a univariate plot that draws a histogram of the data distribution and fits a Kernel Density Estimate (KDE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Density Plot\n",
    "if not os.path.exists(\"plots/density_plot\"):\n",
    "    os.makedirs(\"plots/density_plot\")\n",
    "\n",
    "# draw a histogram and fit a kernel density estimate (KDE)\n",
    "for i, col in enumerate(cols):\n",
    "    if col == \"CHAS\":\n",
    "        pass  # We do this because it is a binary data and KDE cannot fit it\n",
    "    else:\n",
    "        sns.distplot(dataset[col], color=random.choice(colors))\n",
    "        plt.savefig(\"plots/density_plot/density_plot_\" + str(i) + \".png\")\n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the density plots, we can see that,\n",
    "* `CRIM`, `AGE`, `B`, and `ZN` have an exponential distribution.\n",
    "* `NOX`, `RM`, and `LSTAT` have a skewed gaussian distribution.\n",
    "* `RAD` and `TAX` have a bimodal distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Scatter Plot**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Scatter plot is used to understand the relationship between two different attributes in the dataset. Below we have compared `PRICE` (target) v/s each of the attributes in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"plots/scatter_plot\"):\n",
    "    os.makedirs(\"plots/scatter_plot\")\n",
    "\n",
    "# bivariate plot between target and different attributes\n",
    "for i, col in enumerate(cols):\n",
    "    if (i == len(cols) - 1):\n",
    "        pass\n",
    "    else:\n",
    "        sns.jointplot(x=col, y=\"PRICE\", data=dataset);\n",
    "        plt.savefig(\"plots/scatter_plot/PRICE_vs_\" + str(i) + \".png\")\n",
    "        plt.show()\n",
    "        plt.clf()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first split our data into training and test sets using the `train_test_split` method from scikit-learn. For this demo, we will use a split of 70% of the data for training and 30% for testing. We also set a `random_state` seed, in order to allow reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = dataset.drop(\"PRICE\", axis=1)\n",
    "y = dataset[\"PRICE\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will normalize our dataset in order to provide a standardized input to our machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = X_train.mean(axis=0)\n",
    "std = X_train.std(axis=0)\n",
    "\n",
    "X_train = (X_train - mean) / std\n",
    "X_test = (X_test - mean) / std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Building our Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demo, we are going to three machine learning models to predict housing prices in Boston."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Linear Regression Model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's try to predict housing prices with the Linear Regression algorithm and see how our model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "mse_lr = mean_squared_error(y_test, y_pred_lr)\n",
    "mae_lr = mean_absolute_error(y_test, y_pred_lr)\n",
    "\n",
    "print('Mean squared error on test data: ', mse_lr)\n",
    "print('Mean absolute error on test data: ', mae_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s see the major features that have an impact on the model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = lr_model.coef_\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "\n",
    "plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "plt.yticks(pos, dataset.columns[sorted_idx])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.title('Variable Importance')\n",
    "plt.savefig(\"plots/feature_importance_lr.png\")\n",
    "plt.show()\n",
    "plt.clf()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Decision Tree**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s try our prediction using the Decision Tree algorithm and see how our model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "tree = DecisionTreeRegressor()\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "y_pred_tree = tree.predict(X_test)\n",
    "\n",
    "mse_dt = mean_squared_error(y_test, y_pred_tree)\n",
    "mae_dt = mean_absolute_error(y_test, y_pred_tree)\n",
    "\n",
    "print('Mean squared error on test data: ', mse_dt)\n",
    "print('Mean absolute error on test data: ', mae_dt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s see the major features that have an impact on the model output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importance = tree.feature_importances_\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "\n",
    "sorted_idx = np.argsort(feature_importance)\n",
    "pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "\n",
    "plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "plt.yticks(pos, dataset.columns[sorted_idx])\n",
    "plt.xlabel('Relative Importance')\n",
    "plt.title('Variable Importance')\n",
    "plt.savefig(\"plots/feature_importance_lr.png\")\n",
    "plt.show()\n",
    "plt.clf()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Artificial Neural Network**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a neural network to predict the housing prices and see how our model performs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Building the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_shape=(13, ), activation='relu', name='dense_1'))\n",
    "model.add(Dense(64, activation='relu', name='dense_2'))\n",
    "model.add(Dense(1, activation='linear', name='dense_output'))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Training our model and Evaluating our model**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will fit our model with both our features and their labels, for a total amount of 100 epochs, separating 5% of the samples (18 records) as a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=100, validation_split=0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have successfully trained our model, let’s see how our model does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_nn, mae_nn = model.evaluate(X_test, y_test)\n",
    "\n",
    "print('Mean squared error on test data: ', mse_nn)\n",
    "print('Mean absolute error on test data: ', mae_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Congratulations!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We successfully built our own machine learning model to predict housing prices in Boston using the famous Boston Housing Price dataset and different machine learning techniques like Linear Regression, Decision Tree and Neural Network."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
